\chapter{A deep learning approach to estimate forward default intensities}

\section{Introduction}
\label{S:1.1}
The first default prediction models appeared forty years ago with the first generation model presented by \citet{altman}. This work led to the so-called Altman Z-score formula which uses accounting data to compute the default probability of a firm in the next two years. However, when used for financial firms, Altman's Z-score formula needs to be used with care  because, as I discuss in this chapter, financial firms have to be treated carefully due to their frequent use of off-balance sheet financing. Twenty years later, a second generation of reduced-form models used econometrical tools such as maximum likelihood, probit, and logit regressions. The major drawback of these models is that they do not provide multi-period forecasts. One innovative recent development is the use of a doubly stochastic Poisson intensity model combined with multiple logit regressions to account for multi-period default probability estimation. This model is proposed by \citet{DSW} in \textit{Multi-period corporate default prediction with stochastic covariates}. Their main contribution over prior work is to take advantage of the explanatory covariates' dynamics in order to estimate the multiperiod likelihood of default. Their model employs firm-specific and macroeconomic data to create a Markov state vector $X_t$ in order to compute independent firm default intensities $\lambda(t)$ and other types of exit intensities $\phi(t)$.
The model proposed in \citet{DSW} is the first one capable of multi-period default probability estimation using time dynamics of covariates $X_t$. The applications of \citet{DSW} are various. We can find them in credit rating by credit rating agencies, banks who want to calculate the minimal amount of capital to be held and other researches analyzing the link between macroeconomic cycles and firm's default probabilities. Covariates used by \citet{DSW} are firm's trailing one-year stock return, Distance-to-Default, trailing one-year return on S\&P500 and three-month Treasury bill rate. Estimating their model on US-listed industrial firms between 1980 and 2004, they find that Distance-to-Default and the current state of the economy have a significant impact on default hazard rates.

\clearpage
The two papers closest to this chapter are from \citet{DSW} and \citet{Duan2012}. The first model uses the doubly stochastic argument to derive multi-period default probabilities. To do so, it requires strong assumptions (e.g. vector autoregressive process) regarding the behavior of the time series of covariates to generate future random values for the covariates. If the process is misspecified, biases are introduced both in the forecasted covariates and in the future default probabilities. Five years later, \citet{Duan2012} show that we can relax the VAR assumption with the use of forward intensities. Their paper explains how we can reduce biases by projecting current event realizations on past data. For convenience, the authors specify intensities as a linear function of state variables. I wish to extend the latter by removing the assumption of linear intensities and use an artificial neural network to estimate the intensities of the Poisson processes governing both default and other exits. 
Machine learning techniques for default probabilities estimation are increasingly drawing attention. \citet{barboza2017machine} test several machine learning models to predict bankruptcy one year prior to the event. They document a substantial improvement in prediction accuracy using the Z-score as well as six complementary financial variables. However, pure data-driven models often lack economic relevance and this is where the forward intensity model can contribute to the literature. The forward intensity model is able to provide multi-period predictions while being supported by an economic and econometric background.

In \citet{DSW}, one of the main assumptions is that the covariates governing both default and other exits intensities follow a high-dimensional vector autoregressive (VAR) process. Using this type of process forces the model to greatly reduce either the number of firms in the sample or the number of state variables explaining firm attributes; if one does not restrict the number of firms or variables in the estimation, the dimension of the model will simply be too high and it will considerably increase computational time. A major step forward made in \citet{Duan2012} is to get rid of the VAR process in order to reduce computational time by using a new reduced-form approach based on a forward intensity model. These forward intensities produce a term structure of bankruptcy probabilities without using any sort of high-dimensional process. Using this method allows the model to incorporate a lot more state variables or individual firms in the sample. Moreover, \citet{Duan2012} state that their model may also improve robustness to misspecification because in a VAR model, estimation of future values are highly sensitive to any bias. On the other side, the forward intensity model approach is a projection of past observations on current realizations, which does not involve random estimation of future values. 

Regarding covariates used, both \citet{Duan2012} and \citet{DSW} estimate their own Distance-to-Default (hereafter DtD). An important aspect to highlight is that Distances-to-Default specified in \citet{DSW} differ from those estimated in \citet{Duan2012} since the former estimate DtD using the variance restriction method (see \citep{Duan2012DTD}) while the latter use the transformed-data maximum likelihood estimation method to account for financial firms. The variance restriction method is a popular way to implement the \citet{Merton1974} model but fails at estimating properly the default point for financial firms. Following the KMV assumption (see \citet{KMV}), the default point in this method is specified as short-term debt plus one half of long-term debt and does not take into account other liabilities. However, it is well-known that financial firms such as banks specify a high portion of their debt as other liabilities. Hence, to include financial firms in the sample, the default point has to be adjusted to the sum of short-term debt, one half of long-term debt and a fraction $\delta$ of other liabilities. \citet{Duan2012} use a maximum likelihood estimation in order to compute the unknown fraction $\delta$. The Appendix provides additional methodological information on the DtD estimation using the variance restriction method and the maximum likelihood estimation. 

To summarize, the forward intensity model requires an assumption to link covariates to intensities. \citet{Duan2012} use a linear assumption and a maximum likelihood to estimate those parameters. This chapter contributes to previous literature by using neural networks to relax the linear assumption of forward intensities. Neural networks allow the estimation of highly non-linear functions without specifying the form of the relationships. The remainder of this chapter is structured as follows. Section \ref{S:1.2} sets up the reduced-form model of default, develops the likelihood function used as loss function later on and describes the neural network approach. Section \ref{S:1.3} discusses summary statistics of the dataset used. Section \ref{sec4} presents the results. Section \ref{S.1.5} concludes. Appendix \ref{appendix1} at the end of the thesis contains several proofs and more details on the Distance-to-Default estimation.


\section{Methodology}
\label{S:1.2}

\subsection{Default model}
\label{SS:2-1}

The model adds to the literature on reduced-form models of default for multiperiod corporate prediction using the doubly stochastic formulation as in \citet{DSW} and \citet{Duan2012} (Duan henceforth). The default's time is modeled as the stopping time 
\begin{equation}
\tau_D = \inf\{t : N_t >0 , M_t = 0\},
\end{equation}
where $N_t$ and $M_t$ are the counting processes governing default and other exits respectively. Similarly, the stopping time for combined exits is denoted by
\begin{equation}
\tau_C = \inf\{t : N_t >0 \wedge M_t > 0\}.
\end{equation}

\noindent We have the following :
\begin{equation*}
 \left.\begin{aligned}
        \text{if the firm exits due to default, } \tau_{Ci} = \tau_{Di},\\
        \text{if the firm does not exits due to default, } \tau_{Ci} < \tau_{Di}.
       \end{aligned}
 \right\}
 \qquad \text{$\Rightarrow \tau_{Ci} \leq \tau_{Di}$}
\end{equation*}

Let us denote by $Z_{it}$ the set of firm-specific variables at time $t$ for the firm $i$ and $Y_t$ the set of macroeconomic variables at time $t$. Let  $t_i^0$ be the first time of entry of firm $i$ in the dataset. The econometrician's information set $\mathscr{F}_t$ at time $t$ is thus
\begin{equation}
    \mathscr{F}_t = \{ Y_s : s \leq t \} \cup \mathscr{G}_{1t}  \cup \mathscr{G}_{2t} ... \cup \mathscr{G}_{Nt},
\end{equation}
where 
\begin{equation}
    \mathscr{G}_{it}  = \{  (1_{\tau_{C_i} < u} , 1_{\tau_{D_i} < u} , Z_{iu}) : t_i^0 \leq u \leq \min(\tau_D, \tau_C, t)  \}.
\end{equation}

\begin{figure}[H]
\label{lifespan}
\centering
    \begin{tikzpicture}[
dot/.style = {circle, fill=black,inner sep=0pt, minimum size=4pt},
every label/.append style = {inner sep=0pt, rotate around={0:(-0.5,1.5)}},
    thick      
                        ]
\draw ( 0,0.2) -- + (0,-0.4) node[below] {Start Data};
\draw (10,0.2) -- + (0,-0.4) node[below] {End Data};
%
\draw[thick] (0,0) -- node[below=2mm] {} + (10,0);
%
    \foreach \p in {0.1, 0.3, 0.4, 0.5, 0.6,0.7, 0.9}
{
    \node[dot] at (10*\p,0) {};
}
    \node[dot] at (10*0.1,0) {};
    \node[dot,label={$t_i^0$}] at (10*0.2,0) {};
    \node[dot,label={$\tau_{D_i}$}] at (10*0.8,0) {};

\end{tikzpicture}
\mycaption{Example of the lifespan of a firm}{Each dot corresponds to a time period where the econometrician gathers firm-specific (if available) and macroeconomic variables. $t^0_i$ is the entry time of the firm $i$ and $\tau_{D_i}$ denotes the default time of firm $i$.}
\label{fig:lifespan}
\end{figure}

In Figure \ref{fig:lifespan}, each dot corresponds to a period. At each period, the econometrician gathers firm-specific variables (DtD, Cash/Total Assets, Net Income/Total Assets, ...) and macroeconomic variables (S\&P500 return, treasury rate). For a particular point in time $t$, the econometrician knows the time series of macroeconomic variables until $t$ irrespective of $t_i^0$, and the time series of firm-specific variables from $t_i^0$ to $\tau_{D_i}$ if $t_i^0 \leq t$ and $t \leq \tau_{D_i}$.
Following \citet{DSW}, the conditional probability of default within $s$ years can be computed as 
\begin{equation} \label{eq:3.6}
\mathbb{P}[ \tau_D <t+s |\mathscr{F}_t] = E_t \left[ \int_t^{t+s}  e^{-\int_t^z (\lambda(u)+\phi(u)) du} \cdot \lambda(z) dz \right].
\end{equation}
The probability of default is a function of intensities $\lambda$ (default) and $\phi$ (other exits). However, these intensities are unknown and unobservable. In \citet{DSW}, the state variables governing Poisson intensities are assumed to follow a specific vector autoregressive (VAR) process. This assumption is relaxed in Duan's paper by using forward intensity rates. Instead of modeling $\lambda_{it}$ and $\phi_{it}$ as some functions of state variables available at time $t$, \citet{Duan2012} propose to deal with $f_{it}(\tau)$ and $g_{it}(\tau)$ directly as functions of state variables available at time $t$ and the forward starting time of interest $\tau$. The analogy to interest rates would be that $\lambda_t$ is the short rate and $f_t(u)$ is the forward rate for horizon $u$. \citet{Duan2012} propose a model to predict corporate defaults at multiple horizons by estimating these forward intensities via maximum likelihood. To do so, they use a linear assumption in the relationship between the variables and the forward intensities (i.e. $f_{it}(\tau) = \exp(\alpha_0(\tau) + \alpha_1(\tau) x_{it,1} + \alpha_2(\tau) x_{it,2} + ... + \alpha_k(\tau) x_{it,k}$). However, it is highly likely that default intensities depend on those covariates in a non-linear way. I propose to use an artificial neural network (ANN) to find the set of weights governing the process $f_{it}$ and $g_{it}$. I show that I am able to capture potential non-linear relationships between the state variables and the forward intensities, which significantly improve forecasts.


\subsubsection{Forward intensities}
\label{SSS:2-1-1}

Since we do not have the exact knowledge of $\lambda$ and $\phi$, \citet{Duan2012} propose to use forward intensity rates. However, the default probability given in equation \ref{eq:3.6} (which depends on spot intensities) needs to be translated into a formula that depends on forward intensities. To do so, we first compute the survival probability as a function of forward combined intensity, which will later be used to compute the probability of default as a function of both combined and default forward intensities. Let us denote $F_{it}(\tau)$ the conditional distribution function of the combined (default and other exits) exit time evaluated at $t+\tau$. Hence, 1-$F_{it}(\tau)$ is the probability of surviving in the interval $[t, t+\tau ]$. Therefore, we have :
\begin{equation}
1-F_{it}(\tau) = \mathbb{E}[e^{-\int_t^{t+\tau} (\lambda(s)+\phi(s)) ds}].
\end{equation}
Next, let us introduce the quantity $\psi_{it}(\tau)$ to be :
\begin{equation} \label{eq:3.8}
\psi_{it}(\tau) \equiv -\frac{\ln(1-F_{it}(\tau))}{\tau} \equiv -\frac{\ln(\mathbb{E}[e^{-\int_t^{t+\tau} (\lambda(s)+\phi(s)) ds}])}{\tau}.
\end{equation}
Reverting equation \ref{eq:3.8} gives :
\begin{equation}\label{eq:psi}
e^{-\psi_{it}(\tau)\cdot\tau} = 1-F_{it}(\tau).
\end{equation}
Where $e^{-\psi_{it}(\tau)\cdot\tau}$ is again the survival probability. We now need to compute $\psi_{it}(\tau)\cdot\tau$. At this point, \citet{Duan2012} make the assumption that $\psi_{it}$ is differentiable and define the forward combined exit intensity as 
\begin{equation} \label{eq:3.10}
g_{it}(\tau) \equiv \frac{F_{it}'}{1-F_{it}}.
\end{equation}
Equation \ref{eq:3.10} comes from the definition of a hazard rate function. Referring \citet{book}, the definition of a hazard rate function is the following:
\begin{definition}[Hazard rate]
Let $\tau$ be a stopping time and $F(T) \equiv \mathbb{P}[\tau \leq T]$ its distribution function. Assume that $F(T) < 1 \forall T$, and that F(T) has a density f(T). The hazard rate function $h$ of $\tau$ is :
\begin{equation*}
h(T) \equiv \frac{f(T)}{1-F(T)}.
\end{equation*}
\end{definition}
A hazard rate is the local arrival probability of a stopping time per time interval. Under suitable regularity conditions, intensities and hazard rates are closely similar. In particular, in our doubly-stochastic framework, hazard rates and intensities are equivalent. Hence, in \citet{Duan2012} and this chapter, $\lambda(t) = h(t)$ and thus the distinction between hazard rates and intensity is not made. \\

\noindent The relation between $g_{it}(\tau)$ and $\psi_{it}(\tau)$  is given by :
\begin{align} \label{gphi}
    g_{it}(\tau) &= \frac{F_{it}'(\tau)}{1-F_{it}(\tau)} \nonumber \\
    &= \psi_{it}(\tau) + \psi_{it}'(\tau)\tau.
\end{align}

\noindent Next, we can compute\footnote{Proofs of the following formulations can be found in Appendix \ref{appendix1}.} the quantity $\psi_{it}(\tau) \tau$ that we were looking for as :
\begin{equation}
    \psi_{it}(\tau) \tau = \int_0^{\tau}g_{it}(s) ds.
\end{equation}

\noindent Hence, the probability of surviving over [t,t+$\tau$] is given by :
\begin{equation} \label{survprob}
    \mathbb{P}[\tau_c > t+\tau |\mathbb{F}_t] = \exp(-\int_0^\tau g_{it}(s) ds).
\end{equation}

The forward default intensity for horizon $\tau$ is defined as the limit for a small time step of the probability of defaulting in this small time step given that the firm survives until the considered horizon. The probability is Bayesian and the forward default intensity denoted $f_{it}(\tau)$ is the following :

\begin{equation}
f_{it}(\tau) \equiv \dfrac{\lim_{\Delta t\to 0} \dfrac{\mathbb{P}[t + \tau < \tau_{Di}=\tau_{Ci} \leq t+ \tau + \Delta t]}{\Delta t}}{e^{-\psi_{it}(\tau)\cdot\tau}}.
\end{equation}

Hence, the probability of defaulting between $t$ and $t+\tau$ is given by :

\begin{equation}\label{probdef}
\int_0^{\tau} e^{-\psi_{it}(s)s} f_{it}(s)ds = \int_0^{\tau} e^{-\int_0^s g_{it}(u) du} f_{it}(s)ds.
\end{equation}


\subsubsection{Likelihood function}
\label{SSS:2-1-2}

In this setup, the likelihood function depends on three types of probabilities (default, other exit and surviving) which themselves depend on two types of intensities (default and other exits). The negative log-likelihood function has to be adjusted to the neural network framework and can be used as an objective function to be minimized as long as we feed mini-batches and not single data points (``online learning''). Mini-batch feeding is a common practice in machine learning and consists in splitting the available data in subsets of fixed size. Next, each backward pass takes one batch to perform a gradient descent to update the parameters of the model. \\

To allow further comparison with \citet{Duan2012},  I employ the same discretization of time: $t = 0, 1, 2,...$ and $\tau = 0, 1, 2,...$ are time sequences of one month increments. Similarly, $f_{it}(\tau)$ and $g_{it}(\tau)$ are forward intensities computed at time $t$ for the period [t+$\tau$, t+$\tau$+1]. The use of the $\tau$ index is to account for multiperiod prediction. When $\tau = 0$, the forward intensity model computes spot intensities. When we set $\tau = 1$, the forward intensity model produces estimates one step ahead, and so forth. I denote $X_{it} = (x_{it,1}, x_{it,2} ,...)$ the set of firm-specific and macroeconomic variables explaining both default and combined exit intensities. As specified in \citet{Duan2012}, $f_{it}(\tau)$ and $g_{it}(\tau)$ are functions of $X_{it}$ and can be specified as any form of function as long as they satisfy the constraints that follow. Since combined exit intensity has to be greater than or equal to default intensity, we need to make sure that the forms specified for $f_{it}(\tau)$ and $g_{it}(\tau)$ satisfy the following conditions : $f_{it}(\tau) \leq g_{it}(\tau)$, $f_{it}(\tau) > 0$, $g_{it}(\tau) > 0$. 

I design two neural networks. One is trained to compute $f_{it}$ and the other is trained to output $h_{it}$ where $g_{it} = f_{it} + h_{it}$. I impose non-negativity on outputs of both models such that the combined exit intensity will never be smaller than the default intensity for all horizons. Let us denote $\lambda$ and $\mu$ the set of parameters (weights) tuned in the neural network for $f_{it}$ and $h_{it}$ respectively. $N^{(\lambda)}$ and $N^{(\mu)}$ represent the output of the neural network for $f_{it}$ and $h_{it}$ respectively. The log-likelihood for prediction horizon $\tau$ is expressed\footnote{Proof of the above formulation can be found in Appendix \ref{appendix1}.} as
\begin{equation} \label{decomp1}
\mathcal{L}(\lambda(s)) = \sum_{i=1}^N \sum_{t=0}^{T-s-1} \mathcal{L}_{i,t}(\lambda(s)), \qquad s = 0,1,...,\tau-1
\end{equation}

\begin{equation} \label{decomp2}
\mathcal{L}(\mu(s)) = \sum_{i=1}^N \sum_{t=0}^{T-s-1} \mathcal{L}_{i,t}(\mu(s)), \qquad s = 0,1,...,\tau-1
\end{equation}
where 
\begin{align} \label{small_lik_f}
\mathcal{L}_{i,t}(\lambda(s)) &= \underbrace{\textbf{1}_{t_{0i} \leq t,\tau_{Ci} > t + s +1}}_\textrm{(1)} \cdot (-N_{it}^{(\lambda)}(s)) \Delta t)\\ \nonumber
&+ \underbrace{\textbf{1}_{t_{0i} \leq t, \tau_{Di}=\tau_{Ci} \leq t+s+1}}_\textrm{(2)} \cdot \ln(1-\exp[-N_{it}^{(\lambda)}(s)\Delta t]) \\
&+ \underbrace{\textbf{1}_{t_{0i} \leq t, \tau_{Di} \neq \tau_{Ci}, \tau_{Ci} \leq t+s+1}}_\textrm{(3)} \cdot (-N_{it}^{(\lambda)}(s)\Delta t) \nonumber,
\end{align}

\begin{align} \label{small_lik_h}
\mathcal{L}_{i,t}(\mu(s)) &= \underbrace{\textbf{1}_{t_{0i} \leq t,\tau_{Ci} > t + s +1}}_\textrm{(1)} \cdot (-N_{it}^{(\mu)}(s) \Delta t)\\ \nonumber
&+ \underbrace{\textbf{1}_{t_{0i} \leq t, \tau_{Di} \neq \tau_{Ci}, \tau_{Ci} \leq t+s+1}}_\textrm{(3)} \cdot \ln(1-\exp(-N_{it}^{(\mu)}(s))\Delta t)).
\end{align}

The likelihoods \eqref{small_lik_f} and \eqref{small_lik_h} are the sum of the products between event indicator functions and their respective occurence probability. I define the indicator function $\textbf{1}_{A<B}$ to be one if $A<B$ or zero otherwise. These likelihoods specify three mutually exclusive indicator functions, defining three cases over the time interval $[t,t+\tau+1]$:
\begin{enumerate}
\item The firm does not exit the sample between t and $t+\tau$ and is classified as surviving. This case is specified as (1) in the likelihood because the combined exit time $\tau_{Ci}$ is not in the interval $[t,t+\tau+1]$.
\item The firm defaults and exits the sample during the interval. This case is specified as (2) because $\tau_{Ci}=\tau_{Di}$ when the firm exits due to default jointly with $\tau_{Di}$ being in the interval $[t,t+\tau+1]$.
\item The firm exits the sample for other reasons. This case is specified in (3) since the stopping time $\tau_{Di} \neq \tau_{Ci}$ jointly with $\tau_{Ci} \leq t+\tau+1$.
\end{enumerate}

As in previous studies, the likelihood functions still exhibit the decomposable property which allows to estimate the model for each horizon of prediction independently.

Since the intensities are directly driven by the covariates, \citet{Duan2012} require an assumption on the mapping from the covariates to these intensities. In \citet{Duan2012} the mapping is made with a linear assumption, whereas in the framework of this chapter, the mapping depends on the whole architecture of the neural network. When the neural network has only one hidden layer of one neuron coupled with an exponential activation function, the model boils down to \citet{Duan2012} since the intensities would be a linear combination of the covariates. As the width and depth of the network increases, we depart more and more from the linear assumption and we allow more non-linearities to be incorporated in these intensities. \\ 

\subsection{Neural Networks}
\label{SS:2-2}

Neural networks can be seen as a very general function to map a given input (in this case firm-specific and macroeconomic variables) into a desired output (forward intensities). 
They learn how to compute the output by tuning weights in order to minimize a given loss function. A neural network is constructed by juxtaposing several hidden layers of neurons. The input of each layer is a data transformation of the output of the previous layer. Initially, the weights of the network are assigned random values. Then, the training process starts and consists of many iterations of a forward pass and a backward pass. The forward pass takes as input a batch of data and computes the loss value; the backward pass then computes the gradient and adjusts the weights of the network based on a learning rate hyperparameter. As an illustration, Figure \ref{fig:net} shows a neural network with 2 hidden layers : 5 neurons in the first layer and 3 neurons in the second layer. In this example, 3 features (inputs) are fed to the network. Each feature is connected to the first hidden layer by a set of weights. The outputs of the first layer are also weighted to produce the inputs of the second hidden layer. Non-linearity is introduced in each node by a non-linear activation function (e.g. sigmoid). Finally, the output of the second hidden layer is aggregated to produce the final output of the model. The neural networks in this chapter are implemented in Python using the library TensorFlow. Approximately one hour of computing time is needed to fit all networks on a 32GB RAM quad core 2.7 GHz computer.\footnote{GPU computing is not necessary in this context as the networks are small.}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
   shorten >=1pt,->,
   draw=black!50,
    node distance=\layersep,
    every pin edge/.style={<-,shorten <=1pt},
    neuron/.style={circle,fill=black!25,minimum size=17pt,inner sep=0pt},
    input neuron/.style={neuron, fill=green!50},
    output neuron/.style={neuron, fill=red!50},
    hidden neuron/.style={neuron, fill=blue!50},
    annot/.style={text width=4em, text centered}
]
    % Draw the input layer nodes
    \foreach \name / \y in {1,...,3}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
    \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-\y-2.5) {};  %%% <-- MODIFIED
    % set number of hidden layers
    \newcommand\Nhidden{2}

    % Draw the hidden layer nodes

    

       \foreach \y in {1,...,5} { %%% MODIFIED (1,...,12 -> 1,...,5, and the next five lines)
           \node[hidden neuron] (H1-\y) at (1*\layersep,-\y*\nodeinlayersep) {$\frac{1}{1+e^{-x}}$};
       }
    \node[annot,above of=H1-1, node distance=1.5cm] (hl1) {Hidden Layer 1};

        \foreach \y in {1,...,3} { %%% MODIFIED (1,...,12 -> 1,...,5, and the next five lines)
           \node[hidden neuron] (H2-\y) at (2*\layersep,-40-\y*\nodeinlayersep) {$\frac{1}{1+e^{-x}}$};
       }
    \node[annot,above of=H2-1, node distance=2.9cm] (hl2) {Hidden Layer 2};
    
    
    
    % Draw the output layer node
    \node[output neuron,pin={[pin edge={->}]right:Output}, right of=H\Nhidden-2] (O) {}; %%% <-- MODIFIED (from H\Nhidden-6 to H\Nhidden-3) 
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
            \path (I-\source) edge (H1-\dest);
    % connect all hidden stuff
    \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
       \foreach \source in {1,...,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
           \foreach \dest in {1,...,3} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
               \path (H\lastN-\source) edge (H\N-\dest);
    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
        \path (H\Nhidden-\source) edge (O);

    % Annotate the layers
    \node[annot,left of=hl1] {Input Layer};
    \node[annot,right of=hl\Nhidden] {Output Layer};
\end{tikzpicture}
    \mycaption{Illustration of a neural network \text{[}5, 3\text{]}}{In this network, there are three input features (green circles), five neurons in the first hidden layer and 3 neurons in the second hidden layer. Each neuron is activated with a sigmoid function.}
    \label{fig:net}
\end{figure}

The use of neural networks can be motivated twofold. First, neural networks are well suited to approximating a function (in our case forward intensities) with the advantage of having different degrees of modularity. By definition, the architecture of the network generates the form of the function approximated. A deeper network allows for more non-linearities in the approximation of the function, at the expense of having more parameters to estimate. For instance, suppose that every observation comes with 12 input features (i.e. $x$ is a vector of shape 1x12), a neural network [5, 3] (i.e. 2 layers neural network with 5 neurons in the first hidden layer and 3 neurons in the second hidden layer) can be viewed as a function computing the output $N_{it}^{(\lambda)}$ in the following way :


\begin{equation*}
N_{it}^{(\lambda)} =  \begin{bmatrix}
\phi_1 (\begin{bmatrix}
\phi_2 (\begin{bmatrix}
x
\end{bmatrix}_{1\times 12} 
\begin{bmatrix}
w_1
\end{bmatrix}_{12\times 5} + 
\begin{bmatrix}
b_1
\end{bmatrix}_{1\times 5})
\end{bmatrix}_{1 \times 5 } 
\begin{bmatrix}
w_2
\end{bmatrix}_{5 \times 3} + 
\begin{bmatrix}
b_2
\end{bmatrix}_{1 \times 3})
\end{bmatrix}_{1 \times 3} 
\begin{bmatrix}
w
\end{bmatrix}_{3 \times 1},
\end{equation*}


\noindent  with the activation functions being for instance the sigmoid function $\phi_1(x) = \phi_2(x) = \frac{1}{1+e^{-x}}$, $x$ being the data input, $w_1$, $w_2$ and $w$ weight matrices and $b_1$ and $b_2$ biases matrices. In this architecture, the number of parameters to estimate is equal to 12x5 + 1x5 + 5x3 + 1x3 + 3x1 = 86. \\
Second, neural networks are well-suited for this chapter because they allow an easy replication of the benchmark model \citet{Duan2012}. More specifically, if the activation function $\phi(x)$ is chosen as being an exponential $\exp(x)$, and the network architecture is [1] (i.e. a single hidden layer with a single neuron), the output $N_{it}^{(\lambda)}$ becomes the linear assumption

\begin{align*}
N_{it}^{(\lambda)} &=  
\phi 
(
\begin{bmatrix}
x
\end{bmatrix}_{1 \times 12} \cdot
\begin{bmatrix}
w
\end{bmatrix}_{12 \times 1} +
\begin{bmatrix}
b1
\end{bmatrix}_{1 \times 1}
)\\
&= \exp(b1 + w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_{12} \cdot x_{12}),
\end{align*}

\noindent The results from this architecture are described in Section 4.


%HERE
\section{Empirical section}
\label{S:1.3}

\subsection{Data}
\label{sec3-1}


The accounting data is taken from the Wharton Research Data Services (WRDS) using the CRSP/Compustat merged database. The macroeconomic data is taken from CRSP, the Federal Reserve Bank Reports and Datastream. The bankrupcty data is taken from the Compustat database, using the DLRSN item for the reason of deletion and the DLDTE item for the date of deletion. DLRSN contains the code that indicates the reason why a company becomes inactive on the database. I consider firms with a DLRSN code 2 (bankruptcy) or 3 (liquidation) to be defaulted, any other DLRSN code as other exits and no DLRSN code as surviving. For additional information on DLRSN and DLDTE codes, I refer to the Wharton WRDS documentation. I focus on the period from 1991 to 2018 to match the accounting data with the bankruptcy data. Using the WRDS database, I download accounting information for every company that has been listed someday on either NYSE, AMEX or NASDAQ between 1991 and 2018. The dataset spans 27 years of data where firms entered and/or exited anywhere in this sample. Using this kind of sample brings a problem of cylindric data : firm's entering/exiting time are not the same for each company. The dataset is represented as a three dimensional matrix with the x-axis being features (i.e. variables), the y-axis being time, and the z-axis being firms. I fill the matrix with missing values for elements where firm $i$ does not exist or already left at time $t$. 
Since neural networks need a lot of data points to be well trained, I choose not to remove firms even if they have a short lifespan. When a firm has a variable completely missing, I drop the whole firm because the likelihood is not specified if a variable is fully missing. However, when the variable is not fully missing but only some data points are not available, I use the last available information before the missing entry. I winsorize all variables at the 2.5 and 97.5 percentile. 
%Before dropping firms, the dataset has in total 12527 surviving firms, 1453 defaulted firms and 17950 firms that exited the sample for other reasons. 
Finally, I standardize all variables by substracting the mean of the variable and dividing the result by its standard deviation. The variables in the test set are also standardized using their respective mean and standard deviation from the training set. Table \ref{tab:observ} shows the number of firms in the three categories for each horizon of prediction $\tau$, which corresponds to the firm-month observations used in the likelihood for horizon $\tau$ (see equations \ref{small_lik_f} and \ref{small_lik_h}). Figure \ref{fig:defsurv} shows the total number of firms that defaulted, survived or exited for other reasons plotted on a year on year basis.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{def_surv_oe_plot.jpg}
    \mycaption{Defaults, exits for other reasons and survivals each year}{The top plot shows the number of defaulted firms, the middle plot shows the number of exits for other reasons and the bottom plot shows the number of firms surviving each year.}
    \label{fig:defsurv}
\end{figure}

\begin{table}[]
    \centering
    \begin{tabular}{l l l l}
    \hline\hline
Horizon & Survivals & Defaults & Other exits \\
    \hline
0 &  2'025'094 & 514 & 10'746\\
%1 &  2'007'311 & 497 & 10'384\\
%2 &  1'989'548 & 483 & 10'173\\
3 &  1'972'063 & 499 & 10'713\\
%4 &  1'953'935 & 511 & 10'743\\
%5 &  1'935'839 & 481 & 10'503\\
6 &  1'918'061 & 499 & 10'392\\
%7 &  1'900'041 & 495 & 10'712\\
%8 &  1'882'058 & 460 & 10'433\\
%9 &  1'864'422 & 468 & 10'611\\
%10 &  1'846'587 & 463 & 10'610\\
%11 &  1'828'772 & 445 & 10'324\\
12 &  1'811'323 & 454 & 10'393\\
%13 &  1'793'797 & 453 & 10'387\\
%14 &  1'776'301 & 433 & 10'037\\
%15 &  1'759'250 & 434 & 10'073\\
%16 &  1'742'157 & 432 & 10'070\\
%17 &  1'725'084 & 416 & 9'770\\
%18 &  1'708'387 & 417 & 9'796\\
%19 &  1'691'651 & 416 & 9'793\\
%20 &  1'674'942 & 391 & 9'434\\
%21 &  1'658'690 & 392 & 9'451\\
%22 &  1'642'418 & 390 & 9'446\\
%23 &  1'626'160 & 381 & 9'183\\
24 &  1'610'242 & 382 & 9'193\\
%25 &  1'594'311 & 381 & 9'189\\
%26 &  1'578'397 & 369 & 8'863\\
%27 &  1'562'910 & 370 & 8'871\\
%28 &  1'547'409 & 370 & 8'867\\
%29 &  1'531'923 & 358 & 8'605\\
%30 &  1'516'819 & 360 & 8'606\\
%31 &  1'501'703 & 359 & 8'601\\
%32 &  1'486'610 & 342 & 8'283\\
%33 &  1'471'935 & 343 & 8'296\\
%34 &  1'457'240 & 343 & 8'295\\
35 &  1'457'240 & 343 & 8'295 \\
\hline
    \end{tabular}
    \mycaption{Number of firms in each category for each horizon of prediction $\tau$}{These correspond to the firm-month observations used in the likelihood for horizon $\tau$.}
    \label{tab:observ}
\end{table}

\citet{leippold} use a theoretical model to show that the most powerful default predictor must incorporate both macroeconomic and accounting data. For more transparency and to allow better comparison with previous literature, I choose to work with a similar set of features than the benchmark model \citet{Duan2012}. Firm-specific values are common to each firm, macroeconomics variables are function of market data. The exhaustive list of variables is the following :

\clearpage 

\begin{enumerate}
\item SP500 : trailing 1-year return on the S\&500 index. 
\item Treasury : 3-month annualized US Treasury bill rate. I use this variable as the risk-free rate $r$ in the model. 
\item CASH/TA : cash and short-term investment divided by total assets. Both quantities are taken from the balance sheet of the company.
\item NI/TA : net income divided by total assets. 
\item SIZE : log of the market equity value divided by the cross-sectional average market equity value. The number of shares outsanding times the stock price gives the market equity value.
\item DtD : the Distance-to-Default is a measure introduced in \citet{Merton1974} to gauge how close a firm is from default. In this chapter, the DtD is estimated using a maximum likelihood taking into account other liabilities of each firm to handle financial firm's bias. It is well-known in the literature that DtD is a significant measure to estimate default probabilities but has to be used jointly with other variables. See the Appendix \ref{appendix1} for additional information regarding the estimation procedure of DtD.
\item M/B : asset market value divded by the total book asset value.
\end{enumerate}

To capture momentum of variables, I also compute one step rolling window differences for each firm-specific variable. These variables are called ``$\Delta$" followed by the name of the variable. They represent whether the firm has been improving or deteriorating with respect to this particular variable comparing to the last period performance. Given this model specification, the $\Delta$ is particularly interesting because if a firm shows many consecutive negative delta values, it means the company is in danger. However, it is also important to look at the level value to compare a defaulted firm with a non-defaulted firm. The intuition tells us that prior default time, a defaulted company should have shown lower level values (for instance, DtD) than a non-defaulted firm.

\subsection{Summary statistics}
\label{sec3-2}

This section depicts the summary statistics of the dataset. Table \ref{tab:meanvar} shows a summary of the mean of each variable for the three categories of firms. Please note that we must be very careful when comparing two means in this table. Comparing means needs to be done with confidence intervals and significance tests, which involves standard deviations. This table is presented to give a rough idea without performing any statistical inference. The table shows that the average defaulted firm is smaller, has a lower DtD, a smaller Market-to-book ratio, loses more money and has less cash than the average surviving firm. The prefixes $\Delta$ in front of the variables stand for the one-lag differences. Finally, Figure \ref{fig:corrmat} shows the correlation matrix for the twelve covariates.

\begin{table}[]
    \centering
    \begin{tabular}{lrrr}
    \hline \hline
                 & Surviving &   Default  & Other exits \\
    \hline
Cash/TA          &  0.1952 &  0.1896  &    0.1851 \\
NI/TA            & -0.0756 & -0.2057  &   -0.129 \\
Size             & -2.7845 & -4.6743  &   -3.621 \\
DtD              & 10.641 &  6.441  &    8.401 \\
MBratio          &  2.3774 &  1.6995  &    2.202 \\
$\Delta$ CASH/TA  & -0.0026 &  0.0023  &   -0.0074 \\
$\Delta$   NI/TA     & -0.0019 & -0.0433  &   -0.0148 \\
$\Delta$  Size      & -0.0305 & -0.5095  &   -0.0939 \\
$\Delta$   DtD       & -0.1366 & -0.7346  &    0.0295 \\
$\Delta$  MBratio   & -0.0285 &  0.0520  &    0.0801 \\
\hline
    \end{tabular}
    \mycaption{Mean of variables for surviving firms, defaulted firms and other exits}{The prefix $\Delta$ stands for a one period lagged difference.}
    \label{tab:meanvar}
\end{table}


\begin{figure}
    \centering
    \includegraphics[scale=0.6]{corrmatrix.jpeg}
    \mycaption{Correlation matrix for firm-specific and macroeconomic covariates}{Red values represent positive correlation coefficients, blue values shows negative correlation coefficients.}
    \label{fig:corrmat}
\end{figure}

\section{Results}
\label{sec4}


As for every neural network, we need to chose the optimal hyperparameters to achieve the highest performance. In my setup, this consists mainly of choosing the architecture of the model (i.e. the number of neurons and layers). To do so, there is currently no other better method than trial and error. However, hyperparameters need to be carefully chosen to not overfit the test set. To avoid any overfitting, I perform a 5-fold cross validation for each horizon of prediction. I cut 15\% of the dataset as test set, all results that I will talk about in this chapter are out-of-sample and performed on the observations from the test set that the model has never seen before. The remaining set of observations is partitioned into smaller subsets so that in every fold of the validation a different subset is used as validation set and the rest is used as training set. Finally, to measure the discriminatory power of the models, I use the Lorenz curve (\citet{lorenz}) and I use the Gini coefficient as a scalar performance measure to aggregate across folds to get the  measure that I use to discriminate the models (see Definition \ref{lorenzdef} (\citet{leippold})). \\

%Cumulative accuracy profile (CAP), also known as power curve, is often employed todetermine the performance of a rating/default prediction model. CAP is only concernedwith rankings and totally ignores the degrees of riskiness. To produce a CAP, we first lineup the obligors ranging from most risky to least risky.  Then, set a percentage and takea group of most risky obligors corresponding to this chosen percentage.  Finally, identifydefaulted obligors in this group and compute the percentage represented by these defaultedones in the population of all defaulted obligors. CAP is a plot that relates the percentageamong defaulted obligors to the percentage among the ranked obligors. In a large sample,the CAP corresponding to a perfect risk ranking model should quickly rise to one and levelat one. In contrast, a completely uninformative risk ranking model will have the CAP plotas a line with the slope equal to one.  Figure 4 presents the CAPs corresponding to thestandard and hierarchical intensity models

\begin{definition}[Lorenz curve]\label{lorenzdef}
The Lorenz curve of a predictor P is the two-dimensional graph
\begin{equation*}
    (\mathbb{P}\{P \leq p\},\mathbb{P}\{P\leq p | Y = 1\}),
\end{equation*}
$\forall p \in (-\infty,+\infty)$.
\end{definition}

The Lorenz curve plots on the x-axis the cumulative percentage of observations against the fraction of defaults on the y-axis.
These curves\footnote{Similar plots exist also under different names (e.g. power curves, cumulative accuracy profiles).} are often used in the literature for default prediction (for instance \citet{leippold}, \citet{Duan2012}). They are different from ROC curves and precision-recall curves because they do not rely on thresholds to discriminate between true positives and false positives. Hence, they are particularly well-suited as performance measure in this model because of the multiperiod framework involved. The idea is that if the model is outputting a false positive for an horizon $\tau$ but the true positive is horizon $\tau+1$, the model should not be too hardly penalized. The ROC and precision-recall curves would treat this as a false positive even though the prediction was not that far from the target. The idea behind the Lorenz curve is to order default probabilities and look how they are distributed across defaulted and non-defaulted firms. We can easily see whether the model is outputting high probabilities for defaulted firms and small probabilities for surviving firms. 

Finally, the Gini coefficient is used as the scalar summary statistic to compare the models. It measures the degree of inequality of the Lorenz curve. A perfect model has a Gini coefficient close to 1 and a poor model has a Gini coefficient of 0 (perfect equality).

\subsection{Model choice}
\label{sec4-1}

The choice of the optimal architecture of the neural network is made using k-fold cross-validation. The average Gini coefficient across all folds for every horizon is used as a comparison tool to determine the best model architecture. If the architecture is too deep, the implicit function computed in the network to output forward intensities incorporates too many parameters and the risk of overfitting is larger, resulting in a lower accuracy measure. Similarly, if the network is not deep enough, the forward intensities are computed using a too simplistic representation and result in a low accuracy measure as well. This is usually known in the machine learning literature as the bias-variance tradeoff. Figure \ref{fig:ginicomp} shows the average Gini coefficient computed on the validation test across all folds of the cross validation for each horizon. I compare the scores obtained with different network architectures with those using the linear assumption. In this setup, one can interpret the architecture model as the non-linearity degree because a higher architecture involves more weights in the output function to be estimated. By disantangling the spaghettis, we can clearly see the bias-variance tradeoff. Increasing the depth of the networks from the simplest neural network [1] to a two layers network [2, 1] increases the Gini coefficients, in particular at mid horizons. This is probably due to the non-linearities introduced via the second layer. Next, it looks like the [3, 2] performs similarly as the [2, 1]; but we clearly see that [5, 3] dominates any other previous models. Finally, increasing the depth again to [10, 5] decreases Gini coefficient at all horizons, suggesting a severe overfitting of the forward intensities function. For the following sections, I will now only show out-of-sample results of the [5, 3] architecture using the test set. 

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{duan_vs_NN.jpeg}
    \mycaption{Gini coefficient : Linear vs Neural Networks}{The yellow line shows the Gini coefficient of the linear model for every horizon. The remaining lines show the Gini coefficient for different neural network architectures. The green line is the best performing model as it exhibits the highest Gini for every horizon.}
    \label{fig:ginicomp}
\end{figure}

\subsection{Performance}
\label{sec4-2}

Figure \ref{fig:perf} shows Lorenz curves for horizons 0, 3, 6, 12, 24 and 35 but all results generalize well to all other horizons. The curves are completely out-of-sample since they are computed on the test set that the model has never seen before. Table \ref{tab:ginicoeff} shows Gini coefficients for both the linear assumption \citet{Duan2012} and for the [5, 3] for the same horizons. Overall, as expected the neural network clearly outperforms the linear assumption, suggesting that the linear assumption from \citet{Duan2012} can be greatly improved by adding non-linearities in the specification of the intensities. Unfortunately, it is difficult to tell which kinds of non-linearities should be taken into account since neural networks are often seen as a black box. However, I will still try to answer this question by looking how the average intensity outputted by the model changes when we change a feature ceteris paribus (see Section \ref{sec4-4} dedicated to sensitivities). Another attempt at answering this question is described in Section \ref{sec4-3}, where I dive into the weights of the network to understand how the output is computed.

For comparison purposes, Figure \ref{fig:lor0duan} exhibits the Lorenz curves of the linear assumption, the replication of the linear assumption in the neural network framework, and the [5, 3] model. The linear assumption and its replication have a similar performance, showing that the neural network ``NN+exp+[1]" is indeed able to replicate the linear framework depicted in \citet{Duan2012}.

\begin{figure*}
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{0.png}
            \caption*{Horizon 0}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.48\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{3.png}
            \caption*{Horizon 3}  
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.48\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{6.png}
            \caption*{Horizon 6}   
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.48\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{12.png}
            \caption*{Horizon 12}   
        \end{subfigure}
                \vskip\baselineskip
        \begin{subfigure}[b]{0.48\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{24.png}
            \caption*{Horizon 24}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.48\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{35.png}
            \caption*{Horizon 35}  
        \end{subfigure}
        \mycaption{Out-of-sample Lorenz Curves for each horizon}{The orange line shows the performance of Duan's model and the red line shows the performance of the [5, 3] neural network.} 
        \label{fig:perf}
    \end{figure*}

\begin{table}[]
    \centering
    \begin{tabular}{lll}
\hline
Horizon	&	[5, 3]	&	Linear	\\
\hline \hline
0	&	0.85	&	0.70	\\
%1	&	0.83	&	0.66	\\
%2	&	0.80	&	0.71	\\
3	&	0.79	&	0.71	\\
%4	&	0.79	&	0.58	\\
%5	&	0.77	&	0.53	\\
6	&	0.76	&	0.64	\\
%7	&	0.78	&	0.61	\\
%8	&	0.77	&	0.68	\\
%9	&	0.70	&	0.58	\\
%10	&	0.76	&	0.57	\\
%11	&	0.76	&	0.70	\\
12	&	0.70	&	0.57	\\
%13	&	0.74	&	0.65	\\
%14	&	0.71	&	0.46	\\
%15	&	0.66	&	0.54	\\
%16	&	0.66	&	0.45	\\
%17	&	0.67	&	0.47	\\
%18	&	0.68	&	0.50	\\
%19	&	0.65	&	0.46	\\
%20	&	0.61	&	0.56	\\
%21	&	0.66	&	0.54	\\
%22	&	0.66	&	0.44	\\
%23	&	0.66	&	0.54	\\
24	&	0.66	&	0.44	\\
%25	&	0.58	&	0.47	\\
%26	&	0.63	&	0.52	\\
%27	&	0.63	&	0.50	\\
%28	&	0.63	&	0.48	\\
%29	&	0.68	&	0.48	\\
%30	&	0.62	&	0.49	\\
%31	&	0.56	&	0.51	\\
%32	&	0.62	&	0.46	\\
%33	&	0.62	&	0.38	\\
%34	&	0.62	&	0.32	\\
35	&	0.57	&	0.39	\\
\hline
    \end{tabular}
    \mycaption{Gini coefficients}{Comparison of the out-of-sample Gini coefficients associated to the [5, 3] neural network and the linear assumption.}
    \label{tab:ginicoeff}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{lor0duan.png}
    \mycaption{Comparison with the benchmark model \citet{Duan2012}}{The green line shows the performance of the [1] neural network (i.e. a single hidden layer with a single neuron) with an exponential activation function. The orange line shows the performance of Duan's model and the red line shows the performance of the [5, 3] neural network.}
    \label{fig:lor0duan}
\end{figure}


\clearpage
\subsection{Computational graph}
\label{sec4-3}

Figure \ref{fig:compgraph} is a representation of a fully trained neural network for the forward default intensity $f$ at horizon 0. Negative weights are drawn as blue lines connecting neurons where orange lines show positive weights. The thicker the line, the higher the weight is in absolute value. Recall that each neuron is activated with a sigmoid function and the biases are not shown on the graph.
In the input layer, all variables seem to be used in the computation of the first layer. In the first layer however, the second neuron presents a higher weight in the network than the others. In return, the inputs connected to the second neuron of the first hidden layer all present low relative weights. Even though the computational graph gives an overview of the neural network and is useful to understand the way the output is computed, it is not trivial to see which variables have more impact on the output. In the following section, I plot sensitivities of each variable in each horizon to better understand the causality of each input.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{compgraphr.png}
    \mycaption{Trained \text{[}5, 3\text{]} Neural Network for forward default intensity at horizon 0}{Negative weights are drawn as blue lines connecting neurons where orange lines show positive weights. The thicker the line, the higher the weight is in absolute value.}
    \label{fig:compgraph}
\end{figure}

\subsection{Sensitivities}
\label{sec4-4}

Neural networks are often seen as black boxes because their outputs are coming from a general function involving many parameters. They are incorporating non-linearities via the layers and the activation functions. Figure \ref{fig:sensi} is an attempt at gauging how the model reacts to a change of an input variable. It plots the average default forward intensity against a shift in the specified variable and is computed the following way:

\begin{enumerate}
    \item Compute the forward default intensity for all the observations in the test set and average the result. Keeping everything else equal, change the value of one feature by an absolute value and feed the updated observations to the network as new inputs and compute the new average forward default intensity. 
    \item Repeat step 1 for all absolute values in some interval.
    \item Plot the average intensity against the absolute change.
    \item Repeat steps 1-3 for all 11 other features.
\end{enumerate}

As explained in Section \ref{SSS:2-1-2}, I impose a non-negativity constraint on forward intensities. A decreasing relationship means that an increase of the associated variable decreases the probability of default of the firm. A flat relationship means that the associated variable has a limited impact on the probability of default. We should expect CASH/TA, size, DtD, NI/TA, Market-to-book ratio and all their lagged differences ($\Delta$) to be decreasing.

First of all, most graphs show non-linear relationships, which is not surprising given the nature of the neural network specification. Moreover, all relationships are intuitive and expected. Forward intensities in both Size and $\Delta$Size are decreasing, suggesting that small firms tend to have a higher likelihood of defaulting, which is consistent with the ``too big to fail" paradigm. Similarly, firms with decreasing cash ($\Delta$CASH/TA $<$ 0) or low levels of CASH/TA appear to have higher probabilities of default. The model also predicts that firms with low NI/TA or decreasing NI/TA should have higher probabilities of default. Finally, forward intensities in DtD should be decreasing for all horizons to reflect that a higher Distance-to-Default makes the firm less likely to default. At horizon 0, a negative change in Distance-to-Default has a substantially greater effect on forward intensities than any other variables. This result is consistent with \cite{DSW}. Overall,
it appears that the most important predictors of default in this model are in the short term Market-to-book ratio, DtD and CASH/TA, and in the long run NI/TA, CASH/TA, $\Delta$CASH/TA and DtD.


\begin{figure*}
\centering
        \begin{subfigure}[b]{\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{legendsensi.JPG}
            {{\small }}    
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{0_corr.png}
            \caption*{Horizon 0}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{3_corr.png}
            \caption*{Horizon 3}    
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{6_corr.png}
            \caption*{Horizon 6}  
            \captionsetup{position=top}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{12_corr.png}
            \caption*{Horizon 12}  
            \captionsetup{position=top}
        \end{subfigure}
                \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{24_corr.png}
            \caption*{Horizon 24}  
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{35_corr.png}
            \caption*{\small Horizon 35}
        \end{subfigure}
        \mycaption{Sensitivities for each horizon}{These plots show the average default forward intensity against a change in the specified variable, ceteris paribus.}
        \label{fig:sensi}
    \end{figure*}



\section{Conclusion}
\label{S.1.5}

I propose an approach to estimate forward default intensities, which relies on using machine learning techniques. The key improvement over previous estimation methods is the introduction of possible highly non-linear relationships between covariates and forward intensities. Neural networks are nothing else than a very general mapping of input data to an output which is obtained by tuning weights while minimizing a given loss function. Non-linearities are introduced via the juxtaposition of layers and the activation functions. The econometric model governing the forward intensity written by \citet{Duan2012} has been adapted to this new framework to allow the use of neural networks. Neural networks are also well-suited for this chapter because they allow an easy replication of the benchmark model \citet{Duan2012}. More specifically, if the network architecture is [1] (i.e. : one layer and one neuron) and the activation function is chosen as being an exponential, the neural network boils down to a logit regression. The dataset used in this chapter is the same as in previous literature to allow for an easier comparison. It consists of 5 firm-specific variables computed from accounting data and 2 macroeconomic variables to control for the health of the economy. I also account for momentum of these variables by feeding the model the one-lagged differences of each variable. Looking at summary statistics only, the average defaulted firm is small, has low cash, low market-to-book ratio, low Distance-to-Default and has large negative profits. To measure the discriminatory power of the models, I follow the previous literature and use Lorenz curves (also known as ``cumulative accuracy profile" or ``power curves"). The idea behind Lorenz curves is to order default probabilities and look at how they are distributed across defaulted and non-defaulted firms. The average Gini coefficient across all folds of the cross-validation is used as a comparison tool to gauge the accuracy of the model. The results show that the architecture [5, 3] (i.e. 2 layers with 5 neurons in the first hidden layer and 3 neurons in the second hidden layer) seems to outperform other architectures. In this setup, one can interpret the architecture as the non-linearity degree since a higher architecture involves more weights in the output function to be estimated. Out-of-sample Lorenz Curves and Gini coefficients show that the neural network approach outperforms the linear assumption for every horizon, suggesting the presence of non-linearities in forward default intensities. Finally, even if neural networks are known to be black boxes, I show how the model reacts to a change of input variables. Most sensitivities plots show non-linear relationships, which is not surprising given the nature of the neural network specification. It appears that the most important predictors of default in this model are in the short term Market-to-book ratio, Distance-to-Default and NI/TA, and in the long run NI/TA, CASH/TA, $\Delta$CASH/TA and Distance-to-Default. Further works could involve more variables in the estimation. In particular, a challenging (due to lack of data) but nonetheless exciting study would be to explore the effect of market sentiment on default intensities.



